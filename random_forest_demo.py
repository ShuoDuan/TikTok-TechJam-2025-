# -*- coding: utf-8 -*-
"""Random Forest Demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GHlOtlUPxlqQ3xgYeBFOv9lnGi0TjBGm
"""

# pip install pandas numpy scikit-learn matplotlib seaborn joblib

from google.colab import drive
# Mount Google Drive
drive.mount('/content/drive')

"""# Import Packages"""

import pandas as pd
import numpy as np
import joblib
import re
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

"""# Feature Engineering For Testing"""

#Count total words in review
def count_words(text):
    if text is None or pd.isna(text):
        return 0
    return len(str(text).split())


def sentiment_words_count(text):
    """Count positive and negative sentiment words"""
    if text is None or pd.isna(text):
        return 0

    text = str(text).lower()

    # Positive sentiment words
    positive_words = ['good', 'great', 'excellent', 'amazing', 'delicious', 'perfect',
                      'wonderful', 'fantastic', 'outstanding', 'love', 'best', 'nice',
                      'awesome', 'brilliant', 'superb', 'magnificent']

    # Negative sentiment words
    negative_words = ['bad', 'terrible', 'awful', 'horrible', 'disgusting', 'worst',
                      'hate', 'disappointing', 'poor', 'unacceptable', 'rude', 'nasty',
                      'terrible', 'pathetic', 'useless']

    sentiment_count = 0
    for word in positive_words + negative_words:
        import re
        pattern = r'\b' + re.escape(word) + r'\b'
        sentiment_count += len(re.findall(pattern, text))

    return sentiment_count


def count_website_words(text):
    """Count website-related keywords (.com, https, etc.)"""
    if text is None or pd.isna(text):
        return 0

    text = str(text).lower()
    website_keywords = ['.com', '.net', '.org', 'http', 'https', 'www.', 'website', 'link']

    count = 0
    for keyword in website_keywords:
        count += text.count(keyword)

    return count


def count_superlatives(text):
    """Count superlative words (both positive and negative)"""
    if text is None or pd.isna(text):
        return 0

    text = str(text).lower()

    # Food-specific superlatives
    superlatives = [
        # Positive superlatives
        'divine', 'heavenly', 'perfectly cooked', 'to die for', 'best', 'most delicious',
        'incredible', 'unbelievable', 'extraordinary', 'phenomenal',

        # Negative superlatives
        'inedible', 'unbearable', 'unacceptable', 'worst', 'terrible', 'disgusting'
    ]

    count = 0
    for superlative in superlatives:
        count += text.count(superlative)

    return count


def count_generic_words(text):
    """Count generic/vague words that may indicate spam"""
    if text is None or pd.isna(text):
        return 0

    text = str(text).lower()
    generic_words = ['good', 'nice', 'ok', 'fine', 'alright', 'decent', 'average']

    count = 0
    for word in generic_words:
        count += text.count(' ' + word + ' ')  # Word boundaries
        # Also check at start/end of text
        if text.startswith(word + ' ') or text.endswith(' ' + word):
            count += 1

    return count


def business_experience_score(text):
    """Count business experience related words"""
    if text is None or pd.isna(text):
        return 0

    text = str(text).lower()
    business_words = ['service', 'food', 'menu', 'price', 'staff', 'quality',
                      'experience', 'restaurant', 'establishment', 'place',
                      'meal', 'dish', 'cuisine', 'dining']

    count = 0
    for word in business_words:
        count += text.count(word)

    return count


def promotional_language_count(text):
    """Count promotional language indicators"""
    if text is None or pd.isna(text):
        return 0

    text = str(text).lower()
    promo_phrases = ["don't miss", "must try", "highly recommend", "special offer",
                     "limited time", "exclusive", "deal", "discount", "promotion",
                     "hurry", "act now", "call now"]

    count = 0
    for phrase in promo_phrases:
        count += text.count(phrase)

    return count


def story_telling_words_count(text):
    """Count story-telling words that may indicate irrelevant content"""
    if text is None or pd.isna(text):
        return 0

    text = str(text).lower()
    story_words = ['meanwhile', 'during', 'then', 'after', 'before', 'suddenly',
                   'finally', 'later', 'yesterday', 'today', 'when i', 'so i',
                   'first', 'next', 'afterwards', 'eventually']

    count = 0
    for word in story_words:
        count += text.count(word)

    return count


def rate_authenticity_score_vectorized(df):
    """Calculate authenticity score for all rows efficiently"""
    authenticity_scores = []

    # Pre-calculate restaurant averages using 'name' column
    restaurant_avg_ratings = df.groupby('name')['rating'].mean().to_dict()
    restaurant_counts = df.groupby('name').size().to_dict()

    for idx, row in df.iterrows():
        restaurant = row['name']
        current_rating = row['rating']

        if restaurant_counts[restaurant] <= 1:
            authenticity_scores.append(0)
            continue

        total_sum = restaurant_avg_ratings[restaurant] * restaurant_counts[restaurant]
        avg_without_current = (total_sum - current_rating) / (restaurant_counts[restaurant] - 1)

        deviation = abs(current_rating - avg_without_current)

        if deviation >= 3:
            authenticity_scores.append(-2)
        elif deviation >= 2:
            authenticity_scores.append(-1)
        else:
            authenticity_scores.append(1)

    return authenticity_scores


def sensory_words_count(text):
    """Count sensory words (taste, texture, appearance)"""
    if text is None or pd.isna(text):
        return 0

    text = str(text).lower()

    sensory_words = [
        # Taste words
        'flavorful', 'tasty', 'delicious', 'savory', 'sweet', 'bitter',
        'spicy', 'bland', 'flavorless', 'aromatic',

        # Texture words
        'crispy', 'crunchy', 'tender', 'chewy', 'creamy', 'smooth',
        'tough', 'soggy', 'dry', 'juicy',

        # Appearance words
        'beautiful', 'appealing', 'colorful', 'presentation',
        'unappetizing', 'messy', 'burnt', 'overcooked'
    ]

    count = 0
    for word in sensory_words:
        count += text.count(word)

    return count


def service_words_count(text):
    """Count service-related words"""
    if text is None or pd.isna(text):
        return 0

    text = str(text).lower()

    service_words = [
        'service', 'waitstaff', 'server', 'bartender', 'host',
        'friendly', 'attentive', 'professional', 'rude', 'ignored',
        'wait time', 'reservation', 'seated', 'prompt', 'slow'
    ]

    count = 0
    for word in service_words:
        count += text.count(word)

    return count


def atmosphere_words_count(text):
    """Count atmosphere/ambiance words"""
    if text is None or pd.isna(text):
        return 0

    text = str(text).lower()

    atmosphere_words = [
        'atmosphere', 'ambiance', 'decor', 'lighting', 'music',
        'romantic', 'cozy', 'lively', 'noisy', 'quiet',
        'crowded', 'empty', 'clean', 'dirty', 'spacious'
    ]

    count = 0
    for word in atmosphere_words:
        count += text.count(word)

    return count


def process_all_features(filename=r'/content/drive/MyDrive/TikTok TechJam 2025/Sample_Data_Final.csv'):
    try:
        # Load data
        df = pd.read_csv(filename)
        print(f"Loaded data with columns: {df.columns.tolist()}")
        print(f"Data shape: {df.shape}")

        # Drop label column if it exists
        if 'label' in df.columns:
            df = df.drop('label', axis=1)
            print("Dropped 'label' column")

        # Fix rating data type
        df['rating'] = pd.to_numeric(df['rating'], errors='coerce')
        df['rating'] = df['rating'].fillna(df['rating'].median())

        # Add numerical features as new columns
        df['word_count'] = df['text'].apply(count_words)
        df['sentiment_words'] = df['text'].apply(sentiment_words_count)
        df['website_words'] = df['text'].apply(count_website_words)
        df['superlatives'] = df['text'].apply(count_superlatives)
        df['generic_words'] = df['text'].apply(count_generic_words)
        df['business_experience'] = df['text'].apply(business_experience_score)
        df['promotional_language'] = df['text'].apply(promotional_language_count)
        df['story_telling_words'] = df['text'].apply(story_telling_words_count)
        df['sensory_words'] = df['text'].apply(sensory_words_count)
        df['service_words'] = df['text'].apply(service_words_count)
        df['atmosphere_words'] = df['text'].apply(atmosphere_words_count)

        # Calculate authenticity score using vectorized approach
        df['authenticity_score'] = rate_authenticity_score_vectorized(df)

        return df

    except FileNotFoundError:
        print(f"Error: File '{filename}' not found.")
        return None
    except Exception as e:
        print(f"Error: {str(e)}")
        return None


if __name__ == "__main__":
    # Process all features
    df = process_all_features()

    if df is not None:
        # Save with both raw data and features to Desktop
        output_path = r'/content/drive/MyDrive/TikTok TechJam 2025/Sample_With_Features.csv'
        df.to_csv(output_path, index=False)
        print(f"Enhanced dataset saved successfully to: {output_path}")
        print(f"Final dataset shape: {df.shape}")
        print(f"Columns: {df.columns.tolist()}")
    else:
        print("Failed to process features.")

"""# Data Preprossessing"""

def process_all_features(df):
    """Feature engineering to add new features based on reviews."""
    # Add the feature engineering code as provided
    # For example, word_count, sentiment_words, etc.

    df['word_count'] = df['text'].apply(count_words)
    df['sentiment_words'] = df['text'].apply(sentiment_words_count)
    df['website_words'] = df['text'].apply(count_website_words)
    # Add all other feature engineering functions similarly
    df['authenticity_score'] = rate_authenticity_score_vectorized(df)
    return df

# Function to load the pre-trained model
def load_model(model_path):
    """Load a saved model from file."""
    return joblib.load(model_path)

# Function to preprocess the data (scaling and encoding)
def preprocess_data(df):
    """Preprocess the data for prediction (Scaling and Label Encoding)."""
    feature_columns = [
        'rating', 'word_count', 'sentiment_words', 'website_words',
        'superlatives', 'generic_words', 'business_experience',
        'promotional_language', 'story_telling_words', 'sensory_words',
        'service_words', 'atmosphere_words', 'authenticity_score'
    ]

    # Check if all feature columns exist in the DataFrame
    missing_features = [col for col in feature_columns if col not in df.columns]
    if missing_features:
        print(f"Error: Missing feature columns in the DataFrame: {missing_features}")
        return None

    # Select features and scale them
    X = df[feature_columns]
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    return X_scaled, scaler

"""# Model Loading & Make Predictions"""

# Function to make predictions
def predict_labels(model, scaler, df):
    """Use the trained model to predict labels for the dataset."""
    # Preprocess the data
    X_scaled, scaler = preprocess_data(df)
    if X_scaled is None:
        print("Preprocessing failed. Exiting.")
        return None

    # Predict with the trained model
    predictions = model.predict(X_scaled)

    # Add numeric predictions to the dataframe
    df['label'] = predictions

    # Map numbers to string labels
    label_map = {
        1: "Spam",
        2: "Advertisement",
        3: "Irrelevant Content",
        4: "Rant",
        5: "Good Review"
    }
    df['LABEL'] = df['label'].map(label_map)

    return df


def main():
    # Upload data from the user
    input_file = "/content/drive/MyDrive/TikTok TechJam 2025/Sample_With_Features.csv"

    # Load the dataset
    try:
        df = pd.read_csv(input_file)
        print(f"Loaded data with shape: {df.shape}")
    except FileNotFoundError:
        print(f"Error: File not found at {input_file}")
        return
    except Exception as e:
        print(f"Error loading file: {e}")
        return

    # Process features
    df = process_all_features(df)
    print("Features processed successfully.")

    # Load the pre-trained model
    model_path = '/content/drive/MyDrive/TikTok TechJam 2025/random_forest_review_classifier.pkl'
    try:
        model_dict = load_model(model_path)
        model = model_dict['model']
        print(f"Model loaded successfully from {model_path}")
    except Exception as e:
        print(f"Error loading model: {e}")
        return

    # Predict labels
    df_with_labels = predict_labels(model, model_dict['scaler'], df)

    if df_with_labels is not None:
        # Save the dataframe with the predicted labels
        output_file = "/content/drive/MyDrive/TikTok TechJam 2025/Random_Forest_Output.csv"
        df_with_labels.to_csv(output_file, index=False)
        print(f"Predicted labels have been saved to {output_file}")
    else:
        print("Failed to add predictions to the dataset.")

if __name__ == "__main__":
    main()