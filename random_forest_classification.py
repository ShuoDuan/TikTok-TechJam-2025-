# -*- coding: utf-8 -*-
"""Random_Forest_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V-LRErbijILCS2Ix3TMsSVfuupDOQVD5
"""

from google.colab import files
uploaded = files.upload()  # choose reviews_labeled_demo.csv

import pandas as pd
fname = next(iter(uploaded))  # first uploaded file
df = pd.read_csv(fname, encoding="latin-1")

print("Shape:", df.shape)
print("Columns:", list(df.columns))
df.head(10)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve, validation_curve
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import time
import random

# Save the model
def save_model(model, scaler, label_encoder, file_path):
    """Save the trained model and preprocessing objects"""
    joblib.dump({
        'model': model,
        'scaler': scaler,
        'label_encoder': label_encoder
    }, file_path)
    print(f"Model saved to {file_path}")

# Set random seeds for true randomness
np.random.seed()  # No fixed seed
random.seed()     # No fixed seed

# Load the dataset
def load_data(file_path):
    """Load the CSV file into a pandas DataFrame"""
    df = pd.read_csv(file_path)
    return df

# Preprocess the data
def preprocess_data(df):
    """Preprocess the data for machine learning"""
    # Make a copy to avoid modifying the original dataframe
    df_processed = df.copy()

    # Check if 'label' column exists before proceeding
    if 'label' not in df_processed.columns:
        print("Error: 'label' column not found in the DataFrame.")
        print(f"Columns in DataFrame: {list(df_processed.columns)}")
        return None, None, None, None

    # Drop rows where 'label' is NaN before mapping and encoding
    df_processed.dropna(subset=['label'], inplace=True)

    # Map numeric labels to the new string categories
    label_mapping = {
        1.0: "Good Review", # Ensure keys are float if they are read as such
        2.0: "Advertisement",
        3.0: "Irrelevant Content",
        4.0: "Spam"
    }

    # Apply the mapping. Labels that don't match the mapping will remain as they are (e.g., "Rant", "Good Review")
    df_processed['label'] = df_processed['label'].map(label_mapping).fillna(df_processed['label'])


    # Encode categorical target variable
    le = LabelEncoder()
    df_processed['label_encoded'] = le.fit_transform(df_processed['label'])

    # Select features and target
    feature_columns = [
        'rating', 'word_count', 'sentiment_words', 'website_words',
        'superlatives', 'generic_words', 'business_experience',
        'promotional_language', 'story_telling_words', 'sensory_words',
        'service_words', 'atmosphere_words', 'authenticity_score'
    ]

    # Check if all feature columns exist in the DataFrame
    missing_features = [col for col in feature_columns if col not in df_processed.columns]
    if missing_features:
        print(f"Error: Missing feature columns in the DataFrame: {missing_features}")
        print(f"Available columns: {list(df_processed.columns)}")
        return None, None, None, None


    X = df_processed[feature_columns]
    y = df_processed['label_encoded']

    # Get mapping for target categories
    category_mapping = dict(zip(le.transform(le.classes_), le.classes_))

    return X, y, category_mapping, le

# Train Random Forest model
def train_random_forest(X, y, test_size=0.2):
    """Train a Random Forest classifier with truly random splits"""
    # Split the data (no random_state for true randomness)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, stratify=None
    )

    # Scale the features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Initialize and train Random Forest (no random_state)
    rf = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        class_weight='balanced'
    )

    rf.fit(X_train_scaled, y_train)

    # Make predictions
    y_pred = rf.predict(X_test_scaled)

    return rf, X_train_scaled, X_test_scaled, y_train, y_test, y_pred, scaler

# Evaluate the model
def evaluate_model(rf, X_test, y_test, y_pred, category_mapping, X_original):
    """Evaluate the Random Forest model"""
    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.4f}")

    # Cross-validation score
    cv_scores = cross_val_score(rf, X_test, y_test, cv=5)
    print(f"Cross-validation scores: {cv_scores}")
    print(f"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

    # Classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=[category_mapping[i] for i in sorted(category_mapping.keys())]))

    # Confusion matrix
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=[category_mapping[i] for i in sorted(category_mapping.keys())],
                yticklabels=[category_mapping[i] for i in sorted(category_mapping.keys())])
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': X_original.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)

    plt.figure(figsize=(12, 8))
    sns.barplot(x='importance', y='feature', data=feature_importance.head(15))
    plt.title('Top 15 Feature Importances')
    plt.show()

    return accuracy, feature_importance

# Hyperparameter tuning
def tune_hyperparameters(X, y):
    """Perform hyperparameter tuning for Random Forest"""
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2']
    }

    rf = RandomForestClassifier(class_weight='balanced')

    grid_search = GridSearchCV(
        estimator=rf,
        param_grid=param_grid,
        cv=5,
        n_jobs=-1,
        scoring='accuracy'
    )

    print("Starting hyperparameter tuning...")
    start_time = time.time()
    grid_search.fit(X, y)
    end_time = time.time()
    print(f"Hyperparameter tuning completed in {end_time - start_time:.2f} seconds")

    print("Best parameters:", grid_search.best_params_)
    print("Best cross-validation score:", grid_search.best_score_)

    return grid_search.best_estimator_

# VALIDATION FUNCTIONS
def perform_cross_validation(model, X, y, cv=5):
    """Perform k-fold cross validation"""
    print(f"\nPerforming {cv}-fold cross-validation...")
    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')

    print(f"Cross-validation scores: {cv_scores}")
    print(f"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

    return cv_scores

def plot_learning_curve(estimator, X, y, cv=5):
    """Plot learning curves to diagnose bias/variance"""
    print("\nGenerating learning curve...")

    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=-1,
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring='accuracy'
    )

    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, 'o-', color='r', label='Training score')
    plt.plot(train_sizes, test_mean, 'o-', color='g', label='Cross-validation score')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='r')
    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='g')

    plt.title('Learning Curve')
    plt.xlabel('Training examples')
    plt.ylabel('Accuracy score')
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()

    return train_sizes, train_scores, test_scores

def plot_validation_curve(estimator, X, y, param_name, param_range):
    """Plot validation curve for a specific parameter"""
    print(f"\nGenerating validation curve for {param_name}...")

    train_scores, test_scores = validation_curve(
        estimator, X, y, param_name=param_name, param_range=param_range,
        cv=5, scoring='accuracy', n_jobs=-1
    )

    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.plot(param_range, train_mean, 'o-', color='r', label='Training score')
    plt.plot(param_range, test_mean, 'o-', color='g', label='Cross-validation score')
    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='r')
    plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.1, color='g')

    plt.title(f'Validation Curve for {param_name}')
    plt.xlabel(param_name)
    plt.ylabel('Accuracy score')
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()

    return train_scores, test_scores

def comprehensive_validation(rf_model, X, y):
    """Perform comprehensive validation of the model"""
    print("=" * 60)
    print("COMPREHENSIVE MODEL VALIDATION")
    print("=" * 60)

    # 1. Cross-validation
    cv_scores = perform_cross_validation(rf_model, X, y, cv=5)

    # 2. Learning curve
    plot_learning_curve(rf_model, X, y, cv=5)

    # 3. Validation curves for key parameters
    plot_validation_curve(rf_model, X, y, 'n_estimators', [10, 50, 100, 200])
    # Exclude None from max_depth range for plotting
    plot_validation_curve(rf_model, X, y, 'max_depth', [5, 10, 20, 30])

    print("Comprehensive validation completed!")

# Load the model
def load_model(file_path):
    """Load a saved model"""
    return joblib.load(file_path)

# Predict on new data
def predict_new_data(model_dict, new_data):
    """Make predictions on new data"""
    model = model_dict['model']
    scaler = model_dict['scaler']
    le = model_dict['label_encoder']

    # Preprocess new data
    new_data_scaled = scaler.transform(new_data)

    # Make predictions
    predictions_encoded = model.predict(new_data_scaled)
    predictions = le.inverse_transform(predictions_encoded)

    # Get prediction probabilities
    probabilities = model.predict_proba(new_data_scaled)

    return predictions, probabilities

# Main execution
def main():
    # Load data
    file_path = "/content/FEoutput222.csv"  # Update with your file path
    df = load_data(file_path)

    if df is None:
        print("Data loading failed. Exiting.")
        return

    print("Dataset shape:", df.shape)
    print("\nFirst few rows:")
    print(df.head())

    # Check what columns we have
    print(f"\nColumns in DataFrame: {list(df.columns)}")

    # Preprocess data - using label as the target
    X, y, category_mapping, le = preprocess_data(df)

    if X is None or y is None:
        print("Preprocessing failed. Exiting.")
        return

    print(f"\nFeatures shape: {X.shape}")
    print(f"Target shape: {y.shape}")
    print(f"Category mapping: {category_mapping}")

    # Show distribution of the target variable
    print("\nLabel distribution:")
    print(df['label'].value_counts())

    # Run multiple experiments to show variability
    # run_multiple_experiments(X, y, category_mapping, n_experiments=3) # This function is not defined, commenting it out

    # Single main experiment
    print("\n" + "=" * 60)
    print("MAIN EXPERIMENT")
    print("=" * 60)

    # Train Random Forest model
    rf_model, X_train_scaled, X_test_scaled, y_train, y_test, y_pred, scaler = train_random_forest(X, y)

    # Evaluate model
    accuracy, feature_importance = evaluate_model(rf_model, X_test_scaled, y_test, y_pred, category_mapping, X)

    print("\nTop 10 most important features:")
    print(feature_importance.head(10))

    # COMPREHENSIVE VALIDATION
    comprehensive_validation(rf_model, X_train_scaled, y_train)

    # Optional: Hyperparameter tuning
    print("\nPerforming hyperparameter tuning...")
    best_model = tune_hyperparameters(X_train_scaled, y_train)

    # Validate the tuned model
    print("\nValidating tuned model...")
    y_pred_tuned = best_model.predict(X_test_scaled)
    accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
    print(f"Tuned model accuracy: {accuracy_tuned:.4f}")

    # Compare with original model
    print(f"Original model accuracy: {accuracy:.4f}")
    print(f"Improvement: {accuracy_tuned - accuracy:.4f}")

    # Save the model
    save_model(best_model, scaler, le, 'random_forest_review_classifier.pkl')

    # Example of making predictions on test data
    print("\nExample prediction on test data:")
    sample_data = X_test_scaled[:5]  # First 5 test samples
    predictions, probabilities = predict_new_data(
        {'model': best_model, 'scaler': scaler, 'label_encoder': le},
        sample_data
    )

    print("Predictions:", predictions)
    print("Probabilities shape:", probabilities.shape)



if __name__ == "__main__":
    main()

